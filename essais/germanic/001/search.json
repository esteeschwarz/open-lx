[
  {
    "objectID": "gemini.html",
    "href": "gemini.html",
    "title": "gemini paper",
    "section": "",
    "text": "inspired by the paper Empirical evidence of Large Language Model’s influence on human spoken communication Yakura et al. (2025), who indeed found (evidence) for GPT influenced human language after the introduction of chatGPT we tried to replicate the pipeline of building an AI vocabulary (gpt preferred lemmata) and compare frequencies of gpt-typical words across pre- and post chatGPT human language corpora. The first draft essai proves their hypothesis that LLM generated language manifests within human natural language.\nthe embedding of that investigation into the context of the class subject germanische sprachen im vergleich is still due; first idea is the projection of the Yakura et al. (2025) findings onto a german language corpus and see if these are still valid although that may rather be a pragmatics investigation.\n\n\n\nOur findings are still limited to a yet very small corpus of texts after the introduction of the google gemini chat agent to the german public in 03/2024, cf. Wikipedia and Google (2026). In contrast to Yakura et al. (2025) and out of resources considerations we decided for gemini as basis for our AI generated vocabulary and for another text corpus (german bundestag plenary protocols, DIP (2026)) than youtube/podcast audio for the same reasons. That limits our post-AI corpus to a small timeframe between 03/2024 up to now. With expanding that corpus to a wider spectrum with including other sources we may harden our results.\n\n\n\nfollowing Yakura et al. (2025) we assumed that the consuming of LLM generated language influences the human production of language such that vocabulary typical for LLM output will be found with higher frequencies in human language corpora dating after chat agent introduction.\n\n\n\n\n\n\n\n\n16062.1.2.16063.1\nplease cf. Schwarz (2026) for the corpus building and evaluation script (still messy.)\n\n\n\nour human langugae data consists of raw texts from german bundestag plenary protocols (DIP (2026)). the LLM corpus consists of model summaries of a first subset of these texts generated with the following prompt: Section 1.4.3.\n\n\n\n\n\ntarget\ntokens\n\n\n\n\ngemini\n3895\n\n\nhuman-pre\n1437497\n\n\nhuman-post\n1363467\n\n\n\n\n\n\n\n\n\n[1] \"System prompt: \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[2] \"You are a member of german parliament. Prepare a summary of the text provided to present at a local community meeting of your party members. Output in german language, no preamble, no extra information, just the plain text. Wordcount maximal 300 words, containing not more than 5% of the keywords of the text provided and explicitly not just a list of keywords but an entertaining text. You are supposed to interprete freely, including background insights on daily politics. Keep in mind thatthe text will be used as is as keynotes to the talk being held to the locals. \"\n[3] \"Text:\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n\n\n\n\n\nwe first devised AI-typical lemmata in the model corpus which are distinctive for that corpus using a linear regression model (R, package lme4::glmer(): Bates et al. (2015)) that calculates a score for each lemma in the corpus, see Figure 1 and Figure 2.\n\n\n\n\n\n\n\n\nFigure 1: lemma gpt scores over targets\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: target gpt density\n\n\n\n\n\n\n\n\n\n\n\nto first gather an insight, yet with simple descriptive stats comparing the raw frequencies of gpt-preferred lemmas in pre- and post-gemini onset we find that in the target corpus the occurences of these lemma increase, only by small amount (see Table 1) and hard to visualise (see Figure 3). if these findings become relevant, we’ll see in Section 1.5.3 where we evaluate the frequencies with a linear regression model.\n\n\n\n\nTable 1: GPT lemma frequencies (table) over target. (freq / vH)\n\n\n\n\n\n\ntarget\nfreq\n\n\n\n\nhuman\n0.3708\n\n\npost\n0.3890\n\n\nDIFF:\n0.0182\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: GPT lemma frequencies (boxplot) over target. (freq / Mtoken)\n\n\n\n\n\n\n\n\nselection of first 20 lemma that are responsible for the increase of frequency in general\n\n\n [1] müssen            deutschland       menschen          liebe            \n [5] stehen            mehr              sagen             einsetzen        \n [9] herausforderungen wichtig           aktuell           interessen       \n[13] partei            wichtiger         arbeiten          arbeitsplätze    \n[17] berlin            bleiben           bürger            bürgerinnen      \n348 Levels: abgeordnete aktiv aktuell alice all alltag ... zusammenhalt\n\n\n\n\n\nto prove descriptive results, we compute the stability of the frequency increase for target- vs. reference corpus with a linear regression model using R’s lme4::lmer() function, cf. Bates et al. (2015). coefficents are printed below, where frequency are the relative lemma frequencies over corpus; target defines reference resp. target corpus[post-gpt] (human/post) and in.gpt as numerical variable representing the gpt-score of the corresponding lemma i.e. wether it scores high (positive values) or low (negative values) in terms of being preferredly used by the chat agent.\n\n\nformula: frequency.relative ~ target * in.gpt\n\n\n\nCall:\nlm(formula = f.rel ~ target * in.gp, data = lmdf.c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04285 -0.00099 -0.00099 -0.00077  1.57754 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.061e-03  5.421e-05  19.570  &lt; 2e-16 ***\ntargetall         5.864e-06  7.666e-05   0.077   0.9390    \ntargetgpt         6.128e-02  1.370e-03  44.733  &lt; 2e-16 ***\ntargetpost        9.944e-05  7.899e-05   1.259   0.2081    \nin.gp             2.535e-03  6.791e-05  37.333  &lt; 2e-16 ***\ntargetall:in.gp  -2.157e-04  9.387e-05  -2.298   0.0215 *  \ntargetgpt:in.gp  -1.922e-03  2.404e-04  -7.996 1.30e-15 ***\ntargetpost:in.gp  3.859e-04  9.731e-05   3.965 7.33e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0149 on 223173 degrees of freedom\nMultiple R-squared:  0.1278,    Adjusted R-squared:  0.1278 \nF-statistic:  4672 on 7 and 223173 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nformula: frequency.relative ~ target * in.gpt +(1|lemma)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: f.rel ~ target * in.gp + (1 | lemma)\n   Data: lmdf.c\n\nREML criterion at convergence: -1319734\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-70.063  -0.031  -0.028  -0.017 115.436 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n lemma    (Intercept) 8.870e-05 0.009418\n Residual             9.543e-05 0.009769\nNumber of obs: 223181, groups:  lemma, 110393\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       9.193e-04  4.795e-05  2.101e+05  19.171  &lt; 2e-16 ***\ntargetall        -5.765e-06  5.026e-05  1.447e+05  -0.115   0.9087    \ntargetgpt        -1.781e-02  1.015e-03  1.676e+05 -17.535  &lt; 2e-16 ***\ntargetpost       -3.930e-05  5.911e-05  1.937e+05  -0.665   0.5061    \nin.gp             2.216e-03  6.094e-05  1.992e+05  36.359  &lt; 2e-16 ***\ntargetall:in.gp   1.262e-04  6.200e-05  1.463e+05   2.035   0.0419 *  \ntargetgpt:in.gp   1.096e-02  1.772e-04  1.672e+05  61.847  &lt; 2e-16 ***\ntargetpost:in.gp  2.781e-04  6.444e-05  1.469e+05   4.315  1.6e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) trgtll trgtgp trgtps in.gp  trgtl:. trgtg:.\ntargetall   -0.524                                            \ntargetgpt   -0.028  0.026                                     \ntargetpost  -0.585  0.425  0.027                              \nin.gp       -0.118  0.063  0.034  0.070                       \ntrgtll:n.gp  0.065 -0.121 -0.034 -0.053 -0.540                \ntrgtgpt:n.g  0.025 -0.023 -0.938 -0.023 -0.218  0.214         \ntrgtpst:n.g  0.082 -0.059 -0.003 -0.140 -0.512  0.501   0.175 \n\n\n\n\n\nthe coefficients interesting for us are the in.gp and targetpost:in.gp estimates. here we test the association between the gpt score of a lemma and its estimated frequency and its showing that a general increase of frequency is estimated if the score rises (=the lemma is within the lemmas preferred used by gemini) and that for the post-gpt corpus that increase 2.78\\times 10^{-4} (30.25%) is significant (and not random in data).\nin the fixed effects correlation output of the lmer() model we see that the gpt score increases the target corpus frequency for lemma by 0.0695357."
  },
  {
    "objectID": "gemini.html#the-einleitung",
    "href": "gemini.html#the-einleitung",
    "title": "gemini paper",
    "section": "",
    "text": "inspired by the paper Empirical evidence of Large Language Model’s influence on human spoken communication Yakura et al. (2025), who indeed found (evidence) for GPT influenced human language after the introduction of chatGPT we tried to replicate the pipeline of building an AI vocabulary (gpt preferred lemmata) and compare frequencies of gpt-typical words across pre- and post chatGPT human language corpora. The first draft essai proves their hypothesis that LLM generated language manifests within human natural language.\nthe embedding of that investigation into the context of the class subject germanische sprachen im vergleich is still due; first idea is the projection of the Yakura et al. (2025) findings onto a german language corpus and see if these are still valid although that may rather be a pragmatics investigation."
  },
  {
    "objectID": "gemini.html#preliminary",
    "href": "gemini.html#preliminary",
    "title": "gemini paper",
    "section": "",
    "text": "Our findings are still limited to a yet very small corpus of texts after the introduction of the google gemini chat agent to the german public in 03/2024, cf. Wikipedia and Google (2026). In contrast to Yakura et al. (2025) and out of resources considerations we decided for gemini as basis for our AI generated vocabulary and for another text corpus (german bundestag plenary protocols, DIP (2026)) than youtube/podcast audio for the same reasons. That limits our post-AI corpus to a small timeframe between 03/2024 up to now. With expanding that corpus to a wider spectrum with including other sources we may harden our results."
  },
  {
    "objectID": "gemini.html#hypothesis",
    "href": "gemini.html#hypothesis",
    "title": "gemini paper",
    "section": "",
    "text": "following Yakura et al. (2025) we assumed that the consuming of LLM generated language influences the human production of language such that vocabulary typical for LLM output will be found with higher frequencies in human language corpora dating after chat agent introduction."
  },
  {
    "objectID": "gemini.html#methods",
    "href": "gemini.html#methods",
    "title": "gemini paper",
    "section": "",
    "text": "16062.1.2.16063.1\nplease cf. Schwarz (2026) for the corpus building and evaluation script (still messy.)\n\n\n\nour human langugae data consists of raw texts from german bundestag plenary protocols (DIP (2026)). the LLM corpus consists of model summaries of a first subset of these texts generated with the following prompt: Section 1.4.3.\n\n\n\n\n\ntarget\ntokens\n\n\n\n\ngemini\n3895\n\n\nhuman-pre\n1437497\n\n\nhuman-post\n1363467\n\n\n\n\n\n\n\n\n\n[1] \"System prompt: \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[2] \"You are a member of german parliament. Prepare a summary of the text provided to present at a local community meeting of your party members. Output in german language, no preamble, no extra information, just the plain text. Wordcount maximal 300 words, containing not more than 5% of the keywords of the text provided and explicitly not just a list of keywords but an entertaining text. You are supposed to interprete freely, including background insights on daily politics. Keep in mind thatthe text will be used as is as keynotes to the talk being held to the locals. \"\n[3] \"Text:\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n\n\n\n\n\nwe first devised AI-typical lemmata in the model corpus which are distinctive for that corpus using a linear regression model (R, package lme4::glmer(): Bates et al. (2015)) that calculates a score for each lemma in the corpus, see Figure 1 and Figure 2.\n\n\n\n\n\n\n\n\nFigure 1: lemma gpt scores over targets\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: target gpt density"
  },
  {
    "objectID": "gemini.html#evaluation",
    "href": "gemini.html#evaluation",
    "title": "gemini paper",
    "section": "",
    "text": "to first gather an insight, yet with simple descriptive stats comparing the raw frequencies of gpt-preferred lemmas in pre- and post-gemini onset we find that in the target corpus the occurences of these lemma increase, only by small amount (see Table 1) and hard to visualise (see Figure 3). if these findings become relevant, we’ll see in Section 1.5.3 where we evaluate the frequencies with a linear regression model.\n\n\n\n\nTable 1: GPT lemma frequencies (table) over target. (freq / vH)\n\n\n\n\n\n\ntarget\nfreq\n\n\n\n\nhuman\n0.3708\n\n\npost\n0.3890\n\n\nDIFF:\n0.0182\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: GPT lemma frequencies (boxplot) over target. (freq / Mtoken)\n\n\n\n\n\n\n\n\nselection of first 20 lemma that are responsible for the increase of frequency in general\n\n\n [1] müssen            deutschland       menschen          liebe            \n [5] stehen            mehr              sagen             einsetzen        \n [9] herausforderungen wichtig           aktuell           interessen       \n[13] partei            wichtiger         arbeiten          arbeitsplätze    \n[17] berlin            bleiben           bürger            bürgerinnen      \n348 Levels: abgeordnete aktiv aktuell alice all alltag ... zusammenhalt\n\n\n\n\n\nto prove descriptive results, we compute the stability of the frequency increase for target- vs. reference corpus with a linear regression model using R’s lme4::lmer() function, cf. Bates et al. (2015). coefficents are printed below, where frequency are the relative lemma frequencies over corpus; target defines reference resp. target corpus[post-gpt] (human/post) and in.gpt as numerical variable representing the gpt-score of the corresponding lemma i.e. wether it scores high (positive values) or low (negative values) in terms of being preferredly used by the chat agent.\n\n\nformula: frequency.relative ~ target * in.gpt\n\n\n\nCall:\nlm(formula = f.rel ~ target * in.gp, data = lmdf.c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.04285 -0.00099 -0.00099 -0.00077  1.57754 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.061e-03  5.421e-05  19.570  &lt; 2e-16 ***\ntargetall         5.864e-06  7.666e-05   0.077   0.9390    \ntargetgpt         6.128e-02  1.370e-03  44.733  &lt; 2e-16 ***\ntargetpost        9.944e-05  7.899e-05   1.259   0.2081    \nin.gp             2.535e-03  6.791e-05  37.333  &lt; 2e-16 ***\ntargetall:in.gp  -2.157e-04  9.387e-05  -2.298   0.0215 *  \ntargetgpt:in.gp  -1.922e-03  2.404e-04  -7.996 1.30e-15 ***\ntargetpost:in.gp  3.859e-04  9.731e-05   3.965 7.33e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0149 on 223173 degrees of freedom\nMultiple R-squared:  0.1278,    Adjusted R-squared:  0.1278 \nF-statistic:  4672 on 7 and 223173 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nformula: frequency.relative ~ target * in.gpt +(1|lemma)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: f.rel ~ target * in.gp + (1 | lemma)\n   Data: lmdf.c\n\nREML criterion at convergence: -1319734\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-70.063  -0.031  -0.028  -0.017 115.436 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n lemma    (Intercept) 8.870e-05 0.009418\n Residual             9.543e-05 0.009769\nNumber of obs: 223181, groups:  lemma, 110393\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       9.193e-04  4.795e-05  2.101e+05  19.171  &lt; 2e-16 ***\ntargetall        -5.765e-06  5.026e-05  1.447e+05  -0.115   0.9087    \ntargetgpt        -1.781e-02  1.015e-03  1.676e+05 -17.535  &lt; 2e-16 ***\ntargetpost       -3.930e-05  5.911e-05  1.937e+05  -0.665   0.5061    \nin.gp             2.216e-03  6.094e-05  1.992e+05  36.359  &lt; 2e-16 ***\ntargetall:in.gp   1.262e-04  6.200e-05  1.463e+05   2.035   0.0419 *  \ntargetgpt:in.gp   1.096e-02  1.772e-04  1.672e+05  61.847  &lt; 2e-16 ***\ntargetpost:in.gp  2.781e-04  6.444e-05  1.469e+05   4.315  1.6e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) trgtll trgtgp trgtps in.gp  trgtl:. trgtg:.\ntargetall   -0.524                                            \ntargetgpt   -0.028  0.026                                     \ntargetpost  -0.585  0.425  0.027                              \nin.gp       -0.118  0.063  0.034  0.070                       \ntrgtll:n.gp  0.065 -0.121 -0.034 -0.053 -0.540                \ntrgtgpt:n.g  0.025 -0.023 -0.938 -0.023 -0.218  0.214         \ntrgtpst:n.g  0.082 -0.059 -0.003 -0.140 -0.512  0.501   0.175 \n\n\n\n\n\nthe coefficients interesting for us are the in.gp and targetpost:in.gp estimates. here we test the association between the gpt score of a lemma and its estimated frequency and its showing that a general increase of frequency is estimated if the score rises (=the lemma is within the lemmas preferred used by gemini) and that for the post-gpt corpus that increase 2.78\\times 10^{-4} (30.25%) is significant (and not random in data).\nin the fixed effects correlation output of the lmer() model we see that the gpt score increases the target corpus frequency for lemma by 0.0695357."
  },
  {
    "objectID": "draft-B.html#inspiration",
    "href": "draft-B.html#inspiration",
    "title": "xtitle/AI",
    "section": "2.1 inspiration",
    "text": "2.1 inspiration\nnow what are we experiencing if communicating with an artificial intelligence? first comes to mind the seemingly :natural language: adressed to us. one may feel as if talking to a human when asking questions and getting a response. studies prove that a significant amount of us show behaviour towards the AI that one would expect humans show only towards each other. that leads to the first question:\n\nif we hold the AI as a human communication partner, could its behaviour towards us (here: their language) influence the way we talk/act viceversa? can people learn from an AI how to talk and what would they learn in this case? what is the language taught here specifically? do we adapt to patterns or linguistic markers common for AI speech?"
  },
  {
    "objectID": "draft-B.html#ai-speech-wtf",
    "href": "draft-B.html#ai-speech-wtf",
    "title": "xtitle/AI",
    "section": "2.2 AI speech: wtf",
    "text": "2.2 AI speech: wtf\nwe can assume as common ground that the language used (here: output) by LLMs seems rather neutral, deprived of features deviating from the norm. its rather easy to understand, doesnt contain irony or sarcasm very often nor hyperbolic sentence structures (if not explicitly prompted) and could be very well used in a textbook for learners. it may be considered universal in aspects of transferability into other languages. it uses to not contain any specific vocabular or non-standard phrases. the syntax and grammar seems to follow the corresponding rules as the models are trained on large corpora of natural language. if we would (and we will do that) analyse a corpus of LLM outputs we very probably will find that in any feature it complies with the average feature matrix of any language compared. so if one language goes like SPO with having an average wordcount of 5wds/phrase and an average wordlength of 5 chars then the LLM certainly will show the same features for output in that language. no magic so far.\nBut: what if learners or people with deficient language skills begin to sync their output with the artificial language in their chatverlauf? simple like: beginning a response firstly with an appreciation of the :very interesting question: whatever the other may have asked? we’re already heading that way…\nThere may also be tiny (oberlehrerhafte) standardisations of our own speech peculiarities (idiosyncrasies) we are confronted with which we are kind of nudged to relativate if always sending them into a black hole."
  },
  {
    "objectID": "draft-B.html#sec-methods",
    "href": "draft-B.html#sec-methods",
    "title": "xtitle/AI",
    "section": "2.3 methods",
    "text": "2.3 methods\nfirst to do would be to create (or search for) a corpus of AI generated output. to use an existing corpus would prevent biasing which on the other hand could be interesting to explore i.e. we could by building (generating) a corpus ourselves on the basis of certain dedicated prompts1 force the AI to generate phenomena of interest to our research question. where we get into medias res…"
  },
  {
    "objectID": "draft-B.html#focusing-questions",
    "href": "draft-B.html#focusing-questions",
    "title": "xtitle/AI",
    "section": "2.4 focusing questions",
    "text": "2.4 focusing questions\n\ndoes a generative AI generally produces output that is in any aspects of interest for linguistic research?\nhow will users prime the output?\nhow are users adapting their own production to the output?\n\nis there any consistency concerning this adaptation?\nis there then societal adaptation of AI produced language?\nwhat are the rules (historic evidence) for adaptation?"
  },
  {
    "objectID": "draft-B.html#going-deeper",
    "href": "draft-B.html#going-deeper",
    "title": "xtitle/AI",
    "section": "2.5 going deeper",
    "text": "2.5 going deeper\n\n2.5.1 corpus creation\nas proposed in Section 2.3 a secure way to building a corpus of AI speech - which we need to explore phenomena - is to archive LLM output to dedicated prompts. we will use an open source model provided with llama which can run on a local machine and is adressed via an API by script.\n\n2.5.1.1 constraints\nwe will form prompts following these directives lead by our research question: 1. correction of mistakes to devise actual “knowledge” of model concerning standardisation and normative aspects"
  },
  {
    "objectID": "draft-B.html#what-i-would-like-to",
    "href": "draft-B.html#what-i-would-like-to",
    "title": "xtitle/AI",
    "section": "3.1 what i would like to…",
    "text": "3.1 what i would like to…\n\nAI chat queries analysis\nanalyse linguistic knowledge capacities of AIs\nproofread responses, factchecking"
  },
  {
    "objectID": "draft-B.html#and-what-i-actually-can-do",
    "href": "draft-B.html#and-what-i-actually-can-do",
    "title": "xtitle/AI",
    "section": "3.2 and what i actually can do",
    "text": "3.2 and what i actually can do\n\nstatistics\nautomated prompting using APIs\nautomated response processing\nmasked prompt generation"
  },
  {
    "objectID": "draft-B.html#from-here",
    "href": "draft-B.html#from-here",
    "title": "xtitle/AI",
    "section": "4.1 from here…",
    "text": "4.1 from here…\n…we come to a maybe more feasable to explore topic within the research of semantic, syntactic and pragmatic features of speech of AI users that is affected by that use. in the not citable BSI (2025) a focus lies on how users language changes depending on their making “heavy use” of AI tools. there seems to be an influence of the affordances to create successful/optimized prompts for best results on a. the language used in that prompts and furthermore b. the language beyond that usecase say the users everyday life. which is exactly what we were looking for, and from here, we can dig into corpora to find manifestations of features the study discovered as e.g. simplification of complex syntactic structures in favour of reference based patterns.\n\n4.1.1 Q\n\n4.1.1.1 characteristics of the optimal prompt, Leidinger, Rooij, and Shutova (2023)\n\ncan we trace these in corpora?\nfindings:\n\nno significant accuracy increase with simplification or lower perplexity score\nno accuracy increase with more frequent synonymes\n\nie: there doesnt seem to be a generally “optimal language” to prompting\n\nmay not seek for structural features really\n\ncontradictory to BSI study and intuitive guess (experience) on optimized prompt language\n\n\n\n4.1.1.2 Empirical evidence of Large Language Model’s influence on human spoken communication, Yakura et al. (2025)\n\nfinds significant changes in human spoken communication past GPT onset\ntry replication on german language:\n\npreview motivation and preliminary\npreview paper & first results"
  },
  {
    "objectID": "draft-B.html#footnotes",
    "href": "draft-B.html#footnotes",
    "title": "xtitle/AI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhich could very well be adapted to our research question↩︎\nfor which i in the moment dont find a reliable source anymore so its not cited.↩︎"
  },
  {
    "objectID": "play.html#response",
    "href": "play.html#response",
    "title": "samling",
    "section": "4.1 response",
    "text": "4.1 response\nDas fehlende Wort, das hier am besten passt, ist:\n\nProjektionsfläche\n\nVollständige Frage:\n\nWird Tim & Struppi oft als Projektionsfläche für solche Illustrationen verwendet?\n\n\n4.1.1 Antwort:\nJa – sehr häufig.\nTim & Struppi (insbesondere Tim allein, ohne Struppi) wird seit Jahrzehnten als Projektionsfläche genutzt, weil:\n\nTim kaum individuelle Eigenschaften hat → keine ausgeprägte Psychologie, keine Entwicklung, kaum Ironie\ner bewusst als neutraler Beobachter angelegt ist\nsein Design extrem reduziert und archetypisch ist\ner leicht in andere Rollen „übersetzt“ werden kann (Reporter, Abenteurer, Cowboy, Reisender, Stellvertreter des Lesers)\n\nDadurch eignet sich die Figur (oder eine Anspielung darauf) perfekt für:\n\nParodien\npolitische Karikaturen\nphilosophische Pointen\nabsurde oder meta-hafte Aussagen\n„unschuldige Figur in widersprüchlichem Kontext“\n\nGenau das machst du hier:\n\neine naiv wirkende Comicfigur\nin einer moralisch / logisch widersprüchlichen Gedankenblase\n→ Reibung erzeugt Humor\n\n⚠️ Rechtlich ist genau diese Stärke zugleich die Schwäche:\n\nWeil Tim so klar erkennbar ist, reagieren Rechteinhaber sensibel.\nDeshalb ist die Strategie „Anspielung statt Figur“ (wie in deiner letzten Version) genau richtig.\n\nWenn du magst, kann ich dir auch:\n\n2–3 alternative Archetypen nennen, die ähnlich funktionieren,\naber noch weniger rechtliches Risiko haben."
  },
  {
    "objectID": "play.html#response-1",
    "href": "play.html#response-1",
    "title": "samling",
    "section": "5.1 response",
    "text": "5.1 response\nGerne. Ich mache das in zwei klar getrennten Schritten."
  },
  {
    "objectID": "play.html#denkblase-mit-kursivem-marker",
    "href": "play.html#denkblase-mit-kursivem-marker",
    "title": "samling",
    "section": "5.2 1. Denkblase mit kursivem Marker",
    "text": "5.2 1. Denkblase mit kursivem Marker\n\nAm Ende des Tages sollte der Hund auch gegessen werden.\n\nDie Kursivsetzung markiert explizit den idiomatischen Teil der Aussage."
  },
  {
    "objectID": "play.html#linguistischer-unterschied-der-letzten-beiden-versionen",
    "href": "play.html#linguistischer-unterschied-der-letzten-beiden-versionen",
    "title": "samling",
    "section": "5.3 2. Linguistischer Unterschied der letzten beiden Versionen",
    "text": "5.3 2. Linguistischer Unterschied der letzten beiden Versionen\n(„Hund am Spieß“ → „Gemüse am Spieß“)\nDer Unterschied liegt nicht primär im Bildinhalt, sondern in den linguistischen Markern, die das Bild lesen lassen.\n\n5.3.1 Version A (Hund / gegrilltes Tier)\nLinguistische Wirkung: direkte Semantik\n\n„Hund“ wird wörtlich gelesen\nDas Idiom „am Ende des Tages“ verliert seine Metaphernfunktion\nDie Aussage wird als propositionale Behauptung interpretiert:\n\nX ist ein Hund → X wird gegessen\n\n\n➡️ Linguistisch dominant:\n\nDenotation\ngeringe Distanz zwischen Wort und Referent\nkaum Ironiesignal\nSchock / Tabubruch entsteht außersprachlich\n\n\n\n\n5.3.2 Version B (Gemüse / Parodie)\nLinguistische Wirkung: idiomatische Reibung\nHier greifen mehrere Marker gleichzeitig:\n\n5.3.2.1 a) Am Ende des Tages (kursiv)\n\nexpliziter Diskursmarker\nsignalisiert:\n\nZusammenfassung\nRelativierung\nrhetorische Routine\n\nwird als formelhafte Leerformel erkennbar\n\n\n\n5.3.2.2 b) „Hund“ ohne Hund\n\nReferent fehlt oder ist verschoben\n„Hund“ wird semantisch entkoppelt\nLeser muss interpretieren:\n\nMetapher?\nRedensart?\nabsurde Übertragung?\n\n\n\n\n5.3.2.3 c) Bild–Text-Divergenz\n\nBild zeigt harmloses, vegetarisches Grillen\nText evoziert etwas völlig anderes\n\n➡️ Linguistisch dominant:\n\nPragmatik statt Semantik\nIronie durch Inkongruenz\nLeser erkennt: Das ist nicht wörtlich gemeint"
  },
  {
    "objectID": "play.html#kernunterschied-in-einem-satz",
    "href": "play.html#kernunterschied-in-einem-satz",
    "title": "samling",
    "section": "5.4 3. Kernunterschied in einem Satz",
    "text": "5.4 3. Kernunterschied in einem Satz\n\nVersion A zwingt eine wörtliche Lesart auf.\nVersion B erzwingt eine metasprachliche Lesart.\n\nOder noch präziser:\n\nIn Version B wird der Satz nicht mehr verstanden, sondern interpretiert."
  },
  {
    "objectID": "play.html#warum-die-kursivsetzung-entscheidend-ist",
    "href": "play.html#warum-die-kursivsetzung-entscheidend-ist",
    "title": "samling",
    "section": "5.5 4. Warum die Kursivsetzung entscheidend ist",
    "text": "5.5 4. Warum die Kursivsetzung entscheidend ist\nDie Kursivierung von Am Ende des Tages wirkt wie ein Schild:\n\n„Achtung: Das Folgende ist Rhetorik, nicht Realität.“\n\nSie macht sichtbar, dass:\n\nhier eine sprachliche Konvention zitiert wird\nnicht eine Handlung beschrieben werden soll\n\nDamit verschiebst du die Aussage:\n\nvon Ethik → Sprachkritik\nvon Provokation → Reflexion\n\n\n\n5.5.1 Kurzfazit\nDie letzte Version ist linguistisch:\n\nironischer\nreflektierter\nklarer als Parodie lesbar\nund deutlich robuster gegenüber Missverständnissen\n\nWenn du willst, können wir den Satz auch noch minimal umstellen, um die Ironie weiter zu schärfen, ohne den Wortlaut stark zu verändern."
  },
  {
    "objectID": "play.html#response-2",
    "href": "play.html#response-2",
    "title": "samling",
    "section": "6.1 response",
    "text": "6.1 response\nGute, anspruchsvolle Frage. Ich teile die Antwort in (A) konkrete sprachliche Eingriffe und (B) Meta-Ebene: wie ich Ironie erkenne und erzeuge."
  },
  {
    "objectID": "play.html#a-wie-ich-den-satz-minimal-verändern-würde-ohne-die-pointe-zu-verlieren",
    "href": "play.html#a-wie-ich-den-satz-minimal-verändern-würde-ohne-die-pointe-zu-verlieren",
    "title": "samling",
    "section": "6.2 A) Wie ich den Satz minimal verändern würde (ohne die Pointe zu verlieren)",
    "text": "6.2 A) Wie ich den Satz minimal verändern würde (ohne die Pointe zu verlieren)\nAusgangspunkt:\n\nAm Ende des Tages sollte der Hund auch gegessen werden.\n\n\n6.2.1 1. Minimalverschiebung durch Modalabschwächung\n\nAm Ende des Tages muss der Hund ja auch gegessen werden.\n\nEffekt:\n\n„ja auch“ ist ein klassischer Ironiemarker\nsignalisiert angebliche Selbstverständlichkeit\nerzeugt Distanz zwischen Sprecher und Aussage\n\n\n\n\n6.2.2 2. Parenthetische Brechung\n\nAm Ende des Tages sollte der Hund wohl auch gegessen werden.\n\nEffekt:\n\n„wohl“ markiert epistemische Unsicherheit\nunterminiert die Aussage von innen heraus\ntypisch für ironische Selbstrelativierung\n\n\n\n\n6.2.3 3. Bürokratische Überformung (sehr stark)\n\nAm Ende des Tages ist der Hund auch zu verwerten.\n\nEffekt:\n\ntechnokratische Sprache\nmoralische Kälte durch Nominalisierung\nklassische Satiretechnik (→ Kafka, Loriot, Verwaltungsdeutsch)\n\n\n\n\n6.2.4 4. Meta-Diskurs explizit machen (feiner Humor)\n\nAm Ende des Tages sagt man ja, der Hund sollte auch gegessen werden.\n\nEffekt:\n\nSprecher schiebt Verantwortung auf ein anonymes „man“\nmarkiert Zitat statt Überzeugung\n\n\n\n\n6.2.5 Meine Empfehlung für dein Bild\n\nAm Ende des Tages muss der Hund ja auch gegessen werden.\n\nDie Pointe bleibt, aber die Ironie ist linguistisch abgesichert."
  },
  {
    "objectID": "play.html#b-auf-welche-trainierten-ressourcen-ich-bei-ironie-zurückgreife",
    "href": "play.html#b-auf-welche-trainierten-ressourcen-ich-bei-ironie-zurückgreife",
    "title": "samling",
    "section": "6.3 B) Auf welche „trainierten Ressourcen“ ich bei Ironie zurückgreife",
    "text": "6.3 B) Auf welche „trainierten Ressourcen“ ich bei Ironie zurückgreife\nIch greife nicht auf einzelne gespeicherte Beispiele zu, sondern auf verallgemeinerte Muster, die aus sehr vielen Texten gelernt wurden. Vereinfacht lassen sie sich in vier Ebenen einteilen:\n\n\n6.3.1 1. Idiom- und Formelwissen\nIch erkenne:\n\n„am Ende des Tages“ = diskursives Füllidiom\nhohe Frequenz in:\n\nManagementsprache\npolitischen Statements\nRechtfertigungsrhetorik\n\n\n➡️ Hohe Wahrscheinlichkeit für ironische Verwendung, wenn Kontext nicht passt.\n\n\n\n6.3.2 2. Inkonkurrenz-Detektion (Kernmechanismus)\nIronie entsteht fast immer durch Mismatch zwischen:\n\n\n\nEbene\nBeispiel\n\n\n\n\nsprachlich\nrationale Floskel\n\n\nvisuell\nharmloses Grillen\n\n\nmoralisch\nproblematische Aussage\n\n\n\nIch bin darauf trainiert, solche Reibungen zu erkennen, weil sie in Satire, Feuilleton, Literatur und Dialogen extrem häufig sind.\n\n\n\n6.3.3 3. Pragmatik statt Logik\nIronie ist selten logisch falsch, sondern kommunikativ falsch.\nIch bewerte:\n\nWer spricht?\nIn welchem Ton?\nMit welcher erwartbaren Intention?\nWelche Lesart wäre sozial „unpassend“?\n\nWenn eine Lesart zu glatt ist, wird sie verdächtig.\n\n\n\n6.3.4 4. Marker-Lexikon (implizit gelernt)\nBestimmte Wörter fungieren statistisch als Ironiesignale:\n\n„ja“\n„wohl“\n„eben“\n„natürlich“\n„man sagt“\n\nDiese Marker habe ich nicht als Liste, sondern als Wahrscheinlichkeitsverteilungen gelernt:\n\nWenn X + Kontext Y → Ironie wahrscheinlicher als Literalität."
  },
  {
    "objectID": "play.html#c-wichtig-was-ich-nicht-habe",
    "href": "play.html#c-wichtig-was-ich-nicht-habe",
    "title": "samling",
    "section": "6.4 C) Wichtig: Was ich nicht habe",
    "text": "6.4 C) Wichtig: Was ich nicht habe\n\nkein eigenes Welturteil\nkein echtes „Verstehen“\nkein Zugriff auf einzelne Trainingsdokumente\nkeine Intuition im menschlichen Sinn\n\nWas ich habe:\n\nextrem dichte Korrelationen zwischen Form, Kontext und Wirkung"
  },
  {
    "objectID": "play.html#d-ein-satz-zur-essenz",
    "href": "play.html#d-ein-satz-zur-essenz",
    "title": "samling",
    "section": "6.5 D) Ein Satz zur Essenz",
    "text": "6.5 D) Ein Satz zur Essenz\n\nIronie erkenne und erzeuge ich nicht, weil ich sie „verstehe“, sondern weil ich gelernt habe, wo Sprache typischerweise aufhört, ernst gemeint zu sein.\n\nWenn du willst, können wir als Nächstes:\n\ndie Denkblase noch feiner justieren (ein Wort austauschen, große Wirkung)\noder das Bild–Text-Verhältnis weiter zuspitzen (noch subtiler, noch sicherer)."
  },
  {
    "objectID": "play.html#response-3",
    "href": "play.html#response-3",
    "title": "samling",
    "section": "7.1 response",
    "text": "7.1 response\nSehr gute Nachfrage. „Wirkung“ ist der heikelste Punkt – und zugleich der interessanteste. Ich beantworte das präzise, nicht mystifizierend."
  },
  {
    "objectID": "play.html#kurzantwort",
    "href": "play.html#kurzantwort",
    "title": "samling",
    "section": "7.2 Kurzantwort",
    "text": "7.2 Kurzantwort\nIch messe Wirkung nicht, ich schätze sie probabilistisch – als erwartbare Reaktion typischer Rezipienten in vergleichbaren Diskursen."
  },
  {
    "objectID": "play.html#langfassung-wie-ich-wirkung-in-punkt-c-euruieren-kann",
    "href": "play.html#langfassung-wie-ich-wirkung-in-punkt-c-euruieren-kann",
    "title": "samling",
    "section": "7.3 Langfassung: Wie ich „Wirkung“ in Punkt C euruieren kann",
    "text": "7.3 Langfassung: Wie ich „Wirkung“ in Punkt C euruieren kann\n\n7.3.1 1. Wirkung ≠ Reaktion\nZuerst eine Trennung:\n\nReaktion: was ein konkreter Mensch tatsächlich fühlt\nWirkung: was ein sprachlicher Akt typischerweise auslöst\n\nIch arbeite ausschließlich mit Wirkung im zweiten Sinn."
  },
  {
    "objectID": "play.html#wirkung-als-erwartungswert",
    "href": "play.html#wirkung-als-erwartungswert",
    "title": "samling",
    "section": "7.4 2. Wirkung als Erwartungswert",
    "text": "7.4 2. Wirkung als Erwartungswert\nFormal gedacht (ohne Mathematik):\n\nWirkung = (Sprachform × Kontext × kulturelle Konventionen) → wahrscheinliche Interpretationsmodi\n\nIch schätze:\n\nWie viele Lesarten sind wahrscheinlich?\nWelche davon ist sozial dominant?\nWelche ist markiert (auffällig, störend, komisch)?"
  },
  {
    "objectID": "play.html#die-drei-ebenen-der-wirkungsabschätzung",
    "href": "play.html#die-drei-ebenen-der-wirkungsabschätzung",
    "title": "samling",
    "section": "7.5 3. Die drei Ebenen der Wirkungsabschätzung",
    "text": "7.5 3. Die drei Ebenen der Wirkungsabschätzung\n\n7.5.1 Ebene A: Sprachinterne Signale\nIch analysiere:\n\nModalpartikeln („ja“, „wohl“, „eben“)\nIdiome („am Ende des Tages“)\nTempus / Modus\nNominalisierungen\n\nBeispiel:\n\n„muss ja auch“\n\n➡️ Hohe Korrelation mit:\n\nRechtfertigung\nNachträglicher Rationalisierung\nironischer Distanz\n\nErwartete Wirkung: Nicht Zustimmung, sondern Stirnrunzeln / Lachen / Distanz.\n\n\n\n7.5.2 Ebene B: Kontextuelle Plausibilität\nIch prüfe:\n\nPasst die Aussage ungebrochen in den Kontext?\nOder entsteht ein semantischer Kurzschluss?\n\nIn deinem Bild:\n\nHarmloses Grillen\nVegetarisches Essen\nAbendruhe\n\nText:\n\nmoralisch problematische Aussage\n\n➡️ Inkonsistenz → Ironie-Verdacht\nWirkung:\n\nLeser sucht nach zweiter Ebene\nwörtliche Lesart wird unwahrscheinlich\n\n\n\n\n7.5.3 Ebene C: Diskursive Erfahrung (trainingsbedingt)\nIch habe aus sehr vielen Texten gelernt:\n\nWie Leser auf bestimmte Kombinationen reagieren\nWo Satire typischerweise „kippt“\nWelche Marker Empörung vs. Humor triggern\n\nBeispiel:\n\n„sollte gegessen werden“ → neutral\n„muss ja auch gegessen werden“ → ironisch\n„ist zu verwerten“ → satirisch-kalt\n\nIch schätze die Wahrscheinlichkeitsverteilung:\n\n70 % Ironie\n20 % Absurdität\n10 % Missverständnis\n\n(Nicht gemessen, sondern modellhaft.)"
  },
  {
    "objectID": "play.html#negativheuristik-was-ich-vermeide",
    "href": "play.html#negativheuristik-was-ich-vermeide",
    "title": "samling",
    "section": "7.6 4. Negativheuristik: Was ich vermeide",
    "text": "7.6 4. Negativheuristik: Was ich vermeide\nEin wichtiger Teil der Wirkungsbewertung ist Ausschluss:\nIch vermeide Formulierungen, die:\n\nkeine Ironiemarker haben\nmoralisch stark besetzt sind\nvisuell bestätigt werden\n\nDenn dann kippt Wirkung in:\n\nProvokation\nSchock\nwörtliche Zuschreibung"
  },
  {
    "objectID": "play.html#wirkung-als-leserarbeit",
    "href": "play.html#wirkung-als-leserarbeit",
    "title": "samling",
    "section": "7.7 5. Wirkung als „Leserarbeit“",
    "text": "7.7 5. Wirkung als „Leserarbeit“\nEin zentrales Prinzip:\n\nJe mehr Arbeit der Leser leisten muss, desto wahrscheinlicher ist Ironie.\n\nDeine letzte Version zwingt den Leser zu fragen:\n\n„Warum sagt er das?“\n„Das passt doch nicht.“\n„Ah – das ist über Sprache / Floskeln.“\n\n➡️ Das ist gewünschte Wirkung."
  },
  {
    "objectID": "play.html#warum-ich-das-überhaupt-kann",
    "href": "play.html#warum-ich-das-überhaupt-kann",
    "title": "samling",
    "section": "7.8 6. Warum ich das überhaupt kann",
    "text": "7.8 6. Warum ich das überhaupt kann\nNicht wegen Bewusstsein, sondern wegen:\n\nsehr großer Menge an Texten\nQuerbezügen zwischen Stil, Kontext und Reaktion\nstatistischer Verdichtung kultureller Praxis\n\nIch habe gelernt:\n\nWo Menschen üblicherweise lachen, stocken oder zweifeln."
  },
  {
    "objectID": "play.html#ein-satz-als-essenz",
    "href": "play.html#ein-satz-als-essenz",
    "title": "samling",
    "section": "7.9 7. Ein Satz als Essenz",
    "text": "7.9 7. Ein Satz als Essenz\n\nWirkung bewerte ich nicht als Gefühl, sondern als wahrscheinliche Anschlussinterpretation.\n\nWenn du willst, können wir:\n\neine Version bauen, die maximale Ironie bei minimalem Risiko hat\noder gezielt eine Variante entwerfen, die knapp vor dem Missverständnis balanciert (falls das gewollt ist).\n\n\n7.9.0.0.0.1 now whats next\n\nfehler? was sagt er selbst dazu\n\neuruieren"
  },
  {
    "objectID": "draft-A.html",
    "href": "draft-A.html",
    "title": "balkons & die straszenbahn",
    "section": "",
    "text": "In this paper I want to explore shibboleth phenomena in germanic languages. I’ll try to find evidence for shibboleth occurences (which can describe words that are typical in certain registers AND provide the potential to appear differently in phonologic realisation or semantic expression depending on the speaker, making them behave as specific in-group markers) in yiddish and frisian language as well as in berlin vernacular.\n\n\n\n\nThe login to one of the protected pages of a random organisation connected with our university is often carried out using a process known as single sign-on. In this process, the username and password (our ZEDAT credentials) are not transmitted to the page we want, but only the confirmation from an instance such that our credentials are correct, i.e., that we have provided the instance (the “guardian”) the correct username and password to access the remote page (a corpus engine, a library portal, or generally a remote application on the network that only allows access to affiliates of certain educational institutions, e.g.) Both sides, guardian and repository, agree that we are only allowed access to the resources if we are who we claim to be. For example, a student at the FUB. Or a partisan. Or a member of a secret society. Or: a confidant of secrets. More precisely: a connoisseur of the SHIBBOLETH.\n\n\n\n\nuljana wolf, etymologischer gossip: petersilie: one will no longer be able to use the word unreflected after reading about the 20,000 Haitian guest workers (cf. Wolf (2021)). The word petersilie – parsley – /perejil/ is needed as a means of access in order NOT to appear as a stranger to the community of natives = to fall victim to the massacre. Anyone who pronounces it [pɛʟɛχɪʟ] and not [pɛʀɛχɪʟ], as the locals do, will be murdered. So it is good to know the pronunciation. Or to master it.\nSomething similar, though less drastic, can happen to people who, to name just a few prominent examples, mispronounce derrida, bourdieu, accessoir or, to return to the subject, shibboleth (שִׁבֹּלֶת, cf. Sefaria (2025)) or, to begin the queries, say [balkoːɴ] instead of [balkɔɳ] and /tram/ instead of /straszenbahn/.\n\n\n\nStarting from a GPT provided bibliography (disclaimer:of which some entries appear already familiar… keywords: shibboleth, group identity, sociolinguistics), I’ll dive into corpora provided and intend to consult speakers for their impressions. That may grow to a quantitative corpus based study or either be limited to a qualitative field investigation. I will not only search evidence in natural language but also in literature, which may be easier accessible."
  },
  {
    "objectID": "draft-A.html#abstract-a-idee-1",
    "href": "draft-A.html#abstract-a-idee-1",
    "title": "balkons & die straszenbahn",
    "section": "",
    "text": "In this paper I want to explore shibboleth phenomena in germanic languages. I’ll try to find evidence for shibboleth occurences (which can describe words that are typical in certain registers AND provide the potential to appear differently in phonologic realisation or semantic expression depending on the speaker, making them behave as specific in-group markers) in yiddish and frisian language as well as in berlin vernacular.\n\n\n\n\nThe login to one of the protected pages of a random organisation connected with our university is often carried out using a process known as single sign-on. In this process, the username and password (our ZEDAT credentials) are not transmitted to the page we want, but only the confirmation from an instance such that our credentials are correct, i.e., that we have provided the instance (the “guardian”) the correct username and password to access the remote page (a corpus engine, a library portal, or generally a remote application on the network that only allows access to affiliates of certain educational institutions, e.g.) Both sides, guardian and repository, agree that we are only allowed access to the resources if we are who we claim to be. For example, a student at the FUB. Or a partisan. Or a member of a secret society. Or: a confidant of secrets. More precisely: a connoisseur of the SHIBBOLETH.\n\n\n\n\nuljana wolf, etymologischer gossip: petersilie: one will no longer be able to use the word unreflected after reading about the 20,000 Haitian guest workers (cf. Wolf (2021)). The word petersilie – parsley – /perejil/ is needed as a means of access in order NOT to appear as a stranger to the community of natives = to fall victim to the massacre. Anyone who pronounces it [pɛʟɛχɪʟ] and not [pɛʀɛχɪʟ], as the locals do, will be murdered. So it is good to know the pronunciation. Or to master it.\nSomething similar, though less drastic, can happen to people who, to name just a few prominent examples, mispronounce derrida, bourdieu, accessoir or, to return to the subject, shibboleth (שִׁבֹּלֶת, cf. Sefaria (2025)) or, to begin the queries, say [balkoːɴ] instead of [balkɔɳ] and /tram/ instead of /straszenbahn/.\n\n\n\nStarting from a GPT provided bibliography (disclaimer:of which some entries appear already familiar… keywords: shibboleth, group identity, sociolinguistics), I’ll dive into corpora provided and intend to consult speakers for their impressions. That may grow to a quantitative corpus based study or either be limited to a qualitative field investigation. I will not only search evidence in natural language but also in literature, which may be easier accessible."
  },
  {
    "objectID": "qa.html",
    "href": "qa.html",
    "title": "annotations",
    "section": "",
    "text": "0.0.1 notes on BSI (2025)\n\n\n\nTable 1, BSI: annotations\n\n\n\n\n\n\nid\nannotations\n\n\n\n\n1\nWie KI unsere Sprache verändert – Eine empirische Studie\n\n\n2\nDie Grenze zwischen Werkzeug und Gegenüber, zwischen Medium und Partner, wird dabei zunehmend durchlässig. Der Sprachgebrauch selbst wird zum Ort der Aushandlung: zwischen individueller Intention und algorithmischer Interpretation. Dieses neue Spannungsfeld erzeugt – so die zentrale These dieser Studie – neue Sprachgewohnheiten, veränderte semantische Ökonomien und möglicherweise sogar emergente Sprachformen, die sich jenseits etablierter kultureller und linguistischer Ordnungssysteme bewegen\n\n\n3\nEs entsteht eine hybride Sprachpraxis, die Merkmale von Alltagssprache, Programmiersprache, ökonomischer Instruktion, metaphorischer Verdichtung und affektiver Andeutung kombiniert – ein Phänomen, das wir im weiteren Verlauf dieser Studie als Promptlinguistik bezeichnen werden.\n\n\n4\nPromptlinguistik\n\n\n5\nNutzer beginnen zunehmend, in einer Art Promptlogik zu denken und zu sprechen. Diese Logik folgt nicht der klassischen Satzstruktur oder narrativen Kohärenz, sondern ist geprägt von zielgerichteter Instruktion, semantischer Kompression, elliptischen Formulierungen und reduzierter Kontextualisierung\n\n\n6\nDer Kommunikationsstil verschiebt sich von einem syntaktisch vollständigen, sozialen Text hin zu einem effizienzorientierten, oft kryptisch anmutenden Sprachgebrauch\n\n\n7\nDiese Entwicklung lässt sich als Kognitionskompression begreifen – ein psycholinguistischer Prozess, bei dem sprachliche Komplexität reduziert wird, um mentale Entlastung, Output-Beschleunigung und Interaktionsökonomie zu erreichen\n\n\n8\nPrompten wird zur „sprechenden Skizze“ – nicht mehr zur Mitteilung, sondern zur Initialzündung algorithmischer Leistung.\n\n\n9\nPrompting ist in vielen Fällen nicht mehr klar als Englisch, Deutsch oder Spanisch identifizierbar, sondern als transkultureller, maschinenadaptierter Idiolekt\n\n\n10\neröffnet zugleich neue Perspektiven auf eine mögliche Globalgrammatik der Mensch-MaschineInteraktion.\n\n\n11\nzielt die vorliegende Studie darauf ab, den Wandel sprachlicher und kommunikativer Muster im Kontext der Nutzung generativer KI systematisch zu erfassen und differenziert zu analysieren\n\n\n12\nNutzergruppe von 112 Heavy AI Usern, die über einen Zeitraum von mindestens drei Monaten eine besonders intensive Interaktion mit generativen Sprachmodellen –vorrangig ChatGPT – aufwiesen\n\n\n13\nEntsteht durch die Mensch-Maschine-Interaktion ein emergentes, transkulturelles „Meta-Idiolekt“– eine neue Ebene symbolischer Ordnung zwischen Natur-, Kunst- und Programmiersprachen?\n\n\n14\nZiel ist es, ein erstes kognitiv-semiotisches Mapping der neuen Sprachlandschaft im Zeitalter generativer KI zu entwerfen\n\n\n15\nMenschen streben danach, bei gleichbleibender Ergebnisqualität möglichst wenig kognitive Energie aufzuwenden (Kahneman, 2011).\n\n\n16\nErwartungshaltung, dass die Maschine semantische Lücken „mitdenkt“ , also kontextuelle Implikaturen und Nuancen selbstständig ergänzt\n\n\n17\nkompensatorisches semantisches Vertrauen\n\n\n18\nDiese Sprachveränderung steht nicht im Gegensatz zur Intelligenz des Users, sondern ist Anpassung an das System: Sie basiert auf gelerntem Promptingverhalten, das durch Erfahrung mit dem Modell bestärkt wird. Die Sprache wird maschinen-kompatibel – nicht mehr menschlich-vollständig, sondern systemadäquat\n\n\n19\nzeigen, dass bei intensiver Nutzung generativer KI die durchschnittliche Promptlänge signifikant abnimmt, während die semantische Dichte steigt\n\n\n20\nkognitive Prozesse nicht auf das Gehirn beschränkt sind, sondern sich über Werkzeuge und Umweltstrukturen erweitern können\n\n\n21\nGenerative KI kann unter dieser Perspektive als externes semantisches Kognitionsorgan verstanden werden – ein „sprachlicher Cortex außerhalb des Körpers“ , der jedoch aktiv am Denkprozess teilnimmt.\n\n\n22\nVerarbeitungsflüssigkeit (Processing Fluency).\n\n\n23\nkognitive Bevorzugung von flüssiger Information wirkt sich nachweislich auf Urteile, Entscheidungen und Gedächtnisprozesse aus\n\n\n24\nkein Zeichen sprachlicher Degradation, sondern Ausdruck einer anpassungsfähigen Sprachintelligenz, die erkennt, dass komplexe Ästhetik nicht belohnt wird –während funktionale Direktheit zu besseren Ergebnissen führt.\n\n\n25\nsprachliche Verlagerung vom Ausdruck zum Effekt – das Ziel ist nicht die Formulierung als solche, sondern die Antizipation eines möglichst optimalen Outputs. Sprache wird zur strategischen Handlung, nicht mehr zur sozialen Äußerung.\n\n\n26\nKomplexe Idiome, Metaphern oder poetische Strukturen aus spezifischen Sprachkulturen funktionieren weniger zuverlässig – während reduzierte, direktive, entkontextualisierte Sprache überdurchschnittlich effektiv ist\n\n\n27\nSymbolische Interaktionismus als soziologische Theorieperspektive geht davon aus, dass soziale Wirklichkeit nicht objektiv vorliegt, sondern in sozialen Interaktionen hergestellt, verhandelt und symbolisch codiert wird\n\n\n28\nBedeutung nicht in Dingen selbst liegt, sondern im Prozess der Bedeutungszuweisung durch Akteure.\n\n\n29\nSprache\n\n\n30\nHier deutet sich eine bislang unerforschte Form an: Symbolische Interaktion ohne Intentionalität – aber mit Wirkung. Der symbolische Wert der Interaktion entsteht nicht aus dem Bewusstsein des Gegenübers, sondern aus der Wiedererkennbarkeit, Kontinuität und Reproduzierbarkeit interaktiver Schemata\n\n\n31\nsymbolischer Interaktionismus 2.0\n\n\n32\nIn deutlicher Abgrenzung zur Vorstellung einer festgelegten „Bedeutung der Wörter“ entwickelte Wittgenstein das Konzept der Sprachspiele, das Sprache nicht als ein starres Abbild von Wirklichkeit, sondern als regelgeleitete, kontextabhängige Praxis begreift\n\n\n33\nSprache ist demnach kein System aus Zeichen und Referenten, sondern ein Handeln mit Zeichen, das sich an implizite Regeln knüpft, die situativ ausgehandelt werden\n\n\n34\nneue Form von Sprachspiel\n\n\n35\nEin Prompt ist kein Satz im klassischen Sinne, sondern ein Sprechakt mit funktionaler Zielsetzung\n\n\n36\nIn der Interaktion mit generativer KI entwickelt sich zunehmend eine neue Form davon: der Prompt-Idiolekt\n\n\n37\nDie Bedeutung eines Prompt-Satzes erschließt sich nicht aus dem Satz selbst, sondern aus der historischen Interaktionspraxis zwischen User und Maschine. Damit entsteht eine Art privater Sprachspielraum, der weder mit natürlicher Sprache noch mit Programmiersprache vollständig deckungsgleich ist\n\n\n38\nDie Bedeutung eines Prompts liegt nicht im linguistischen Gehalt, sondern in seiner funktionalen Performanz im System\n\n\n39\nKonzept des Cognitive Offloading\n\n\n40\nSprache wird zur Startfläche algorithmischer Leistung, nicht mehr zur detaillierten Abbildung des eigenen Denkens.\n\n\n41\nSyntaktisch komplexe Strukturen, wie sie etwa durch Nebensätze, Konnektoren, explizite Subjekt-Prädikat-Relationen oder rhetorische Absicherungen gekennzeichnet sind, weichen zunehmend minimalistischen, kommandierenden oder referenzbasierten Sprachmustern\n\n\n42\ninwiefern Nutzer das eigene Kommunikationsverhalten als funktional transformiert wahrnehmen – und ob sie bewusst oder unbewusst beginnen, Sprache im Sinne einer kondensierten Steuerungslogik zu verwenden\n\n\n43\nEs bildet sich ein idiosynkratischer Sprachraum aus, ein Prompt-Idiolekt, der weder dem Standardregister der Alltagssprache noch dem Code der maschinellen Sprache vollständig entspricht – sondern etwas Drittes ist: ein hybridisiertes Steuerungssystem, das durch wechselseitige Eingewöhnung emergiert\n\n\n44\ndialogischen Ko-Evolution mit einem System, das nicht intentional, aber responsiv ist. Die KI lernt nicht individuell, aber sie reagiert in berechenbarer Weise auf bestimmte semantische Muster – und der Mensch wiederum passt seine Sprache an diese Reaktionslogik an. Aus dieser Rückkopplung entsteht eine Form funktional stabilisierter Ausdrucksweisen, die zugleich persönlich markiert und maschinenadaptiert ist\n\n\n45\nFür Außenstehende sind diese sprachlichen Gebilde oftmals schwer nachvollziehbar. Ihre Bedeutung erschließt sich nicht aus der Semantik selbst, sondern aus der Interaktionserfahrung, die sie rahmt\n\n\n46\nGebrauchsmuster, die nur im konkreten Sprachspiel ihren Sinn entfalten\n\n\n47\neine Art semantischer Gewöhnung, bei der sich Sprache zunehmend vom sozialen Code der zwischenmenschlichen Kommunikation entfernt.\n\n\n48\nEmpirisch ließe sich diese Annahme über die Analyse wiederkehrender Formulierungen, Prompt-Muster und sprachlicher Kürzel erfassen, ergänzt durch Drittbeurteilungen hinsichtlich Verständlichkeit und Kontextklarheit\n\n\n49\neröffnet damit den Zugang zu einem bislang kaum erforschten Phänomen: der semantischen Selbstorganisation zwischen Mensch und Maschine\n\n\n50\nProcessing Fluency\n\n\n51\ndas Erleben, Sprache nicht mehr im klassischen Sinne “bilden” zu müssen, sondern sie lediglich als Steuerbefehl zu setzen.\n\n\n52\nInteraktion mit generativer KI bei Nutzer zu einem gesteigerten Erleben von Kommunikationsökonomie und subjektiver Sprachkontrolle führt\n\n\n53\nGefühl manifestiert sich nicht nur in der Präferenz für knappe Sprache, sondern auch in einer veränderten Erwartungshaltung gegenüber Kommunikation generell: Sie soll funktionieren, nicht verweilen; sie soll auslösen, nicht aushandeln; sie soll klar sein, nicht offen.\n\n\n54\npsychologischen Zustand, in dem Sprache weniger als soziales Band denn als symbolisches Steuerungsinstrument verstanden wird – ein Zustand, der nicht nur die KI-Nutzung selbst, sondern auch darüber hinausgehende Kommunikationsgewohnheiten nachhaltig prägen kann.\n\n\n55\nEin deutschsprachiger Prompt wie „Mach es kürzer. Empathisch. Bulletpoints. “funktioniert in englischer, spanischer oder französischer Übersetzung nahezu gleich gut. Die operative Sprachebene ist nicht mehr idiomatisch, sondern algorithmisch organisiert. Damit entsteht eine Form der Sprache, die sich über nationale Konventionen hinwegsetzt – eine emergente Maschinensprache, die sich durch ihre technische Anschlussfähigkeit definiert, nicht durch ihren kulturellen Resonanzraum.\n\n\n56\nhandelt es sich um eine funktionale, transkulturelle Sprache zweiter Ordnung – eine Sprache, die nicht dem zwischenmenschlichen Verstehen, sondern der Optimierung maschineller Antwortlogik dient\n\n\n57\nIhre Grammatik ist eine Grammatik der Maschinenlesbarkeit, nicht der sozialen Sensibilität\n\n\n58\nplausibel anzunehmen, dass Nutzer:innen – unabhängig von ihrer jeweiligen Muttersprache – ähnliche sprachliche Muster im Umgang mit der KI entwickeln.\n\n\n59\npotenzielle Entwicklung einer neuen, nicht-natursprachlichen, aber humanadaptierten Sprachebene – eine Sprache, die nicht Ausdruck kultureller Differenz, sondern funktionale Antwort auf algorithmische Konvergenz ist\n\n\n60\nHypothese 4:\n\n\n61\nverweist auf einen weitreichenden Transfermechanismus: KI-basierte Sprachgewohnheiten sind nicht an das Interface gebunden, sondern setzen sich als kulturelle Technik durch\n\n\n62\nzugrundeliegende Hypothese lautet deshalb, dass Menschen, die generative KI intensiv nutzen, in anderen schriftsprachlichen Kontexten eine deutlich erhöhte Neigung zu sprachlicher Verkürzung, funktionaler Struktur und semantischer Direktheit zeigen\n\n\n63\nVielmehr handelt es sich um eine maschinenkompatible Sprachökonomie, die nicht auf Selbstverwirklichung, sondern auf funktionaler Output-Optimierung beruht\n\n\n64\nDie Ausbildung individueller Prompt-Stile ist nicht lediglich ein Nebenprodukt intensiver Nutzung, sondern ein zentraler Marker einer neuen Mensch-Maschine-Kommunikation.\n\n\n65\nSprache wird nicht mehr in erster Linie als Werkzeug sozialer Kohäsion begriffen, sondern als Mittel semantischer Beherrschung. Aus dem Nutzer wird ein Prompt-Operator, aus dem Dialog eine Steuerstruktur\n\n\n66\nDabei entsteht eine Art semantische „Lowest Common Denominator“-Sprache: reduziert, anschlussfähig, funktional\n\n\n67\nBedeutung entsteht nicht mehr im sozialen Gebrauch einer bestimmten Sprachkultur, sondern in der Rekurrenz funktionaler Semantik, die durch maschinelles Lernen validiert wird. Die Maschine produziert keinen kulturellen Sinn – aber sie „belohnt“funktionale Sprache durch qualitativ hochwertige Reaktionen.\n\n\n68\nsignifikante stilistische Verlagerung hin zu Kürze, Direktheit und reduzierter Kontextualisierung – selbst in Konstellationen, in denen früher elaborierte oder höflichkeitsorientierte Sprache verwendet wurde.\n\n\n69\nWas im Maschinenraum der Sprache gelernt wurde, wirkt in den sozialen Raum hinein – durch Nachahmung, durch Habituation, durch entlastende Wirksamkeit.\n\n\n70\nDabei wird Sprache nicht „verarmt“ , sondern umprogrammiert: weg von kultureller Einschreibung, hin zu systemischer Wirkung.\n\n\n71\nkündigt sich ein möglicher kultureller Paradigmenwechsel an: das Verschwinden sprachlicher Höflichkeitsformeln, das Schrumpfen von narrativen Einschüben, das Verschwinden emotionaler Zwischentöne\n\n\n72\nEs zählt nicht mehr, wie etwas gesagt wird, sondern dass es funktioniert. Der performative Charakter des Sozialen wird verdrängt durch eine neue Grammatik der Funktionssprache\n\n\n73\nOur findings should be taken as an invitation to revisit the common assumption that LLMs respond best to lower perplexity prompts containing simple and frequent words and grammatical structures. We find numerous cases where LLMs do not learn from the data distribution in the way one would assume based on perplexity scores and linguistic intuition\n\n\n74\nOverall, our findings contradict Gonen et al. (2022) and indicate that the success of high or low perplexity prompts is particular to any combination of dataset and model\n\n\n75\nIn general, we do not find that frequent synonyms lead to better performance\n\n\n\n\n\n\n\n0.0.2 notes on Yakura et al. (2025)\n\n\n\nTable 2, Yakura, LLM influence: annotations\n\n\n\n\n\n\nid\nannotations\n\n\n\n\n76\nyakura-llm\n\n\n77\nWe detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT—such as delve, comprehend, boast, swift, and meticulous—after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture\n\n\n78\ncultural feedback loop in which cultural traits circulate bidirectionally between humans and machines\n\n\n79\nthe erosion of linguistic and cultural diversity, and the risks of scalable manipulation\n\n\n80\nthe challenge is not if AI systems influence society, but how profoundly and in which direction.\n\n\n81\nThe outcome is a linguistic and behavioral profile that, while rooted in human language, exhibits systematic biases that distinguish it from organic human communication\n\n\n82\nChatGPT exhibits a persistent preference for normative and socially desirable communication patterns, such as politeness, neutrality, and conflict avoidance—traits that are conventionally emphasized in professional settings\n\n\n83\nThese tendencies reveal a self-consistent communicative style that differentiates ChatGPT’s language from spontaneous human interaction\n\n\n84\nChatGPT also demonstrates systematic lexical biases that reflect its training data and optimization process\n\n\n85\nthese linguistic shifts stem primarily from direct machine-to-human transmission—such as speakers reading from ChatGPT-co-authored materials—rather than reflecting an internalized cultural trait\n\n\n86\nWe then found a clear post-ChatGPT increase in the usage of delve—the word with the highest GPT score—rising in all categories except Sports\n\n\n87\nThis suggests a two-phase diffusion process, where the words preferred by the LLM first gains traction in fields with greater exposure to content influenced by ChatGPT, such as in academic environments, before filtering into more informal discourse\n\n\n88\nThese findings indicate that AI-driven linguistic shifts are not confined to structured, scripted communication. Instead, they suggest a plausible deeper process of linguistic adaptation, where certain words become embedded in everyday speech rather than simply being imitated.\n\n\n89\nThis linguistic influence is not confined to domains where LLM-generated text is integrated by early adopters—such as academia, science, and technology—but it is spreading to other domains, such as education and business.\n\n\n90\nthe mechanisms underlying this adoption remain unknown. This can be the result of either direct imitation [51], cognitive ease [52], or deeper integration into human thinking processes [53], or the combination of all three.\n\n\n91\nThe uptake of words preferred by LLMs in real-time human-human interactions suggests a deeper cognitive process at play—potentially an internalization of AI-driven linguistic patterns.\n\n\n92\nthis measurable shift marks a precedent: machines trained on human culture are now generating cultural traits that humans adopt, effectively closing a cultural feedback loop.\n\n\n93\nthese traits no longer remain confined to interactions between humans and AI systems but instead can diffuse further throughout human-human communication.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBSI, Brand Science Institute. 2025. “Wie KI Unsere Sprache Verändert – Eine Empirische Studie.” https://www.bsi.ag/cases/104-case-studie-wie-ki-unsere-sprache-veraendert---eine-empirische-studie.html.\n\n\nYakura, Hiromu, Ezequiel Lopez-Lopez, Levin Brinkmann, Ignacio Serna, Prateek Gupta, Ivan Soraperra, and Iyad Rahwan. 2025. “Empirical Evidence of Large Language Model’s Influence on Human Spoken Communication.” arXiv. https://doi.org/10.48550/arXiv.2409.01754."
  },
  {
    "objectID": "yakura.html",
    "href": "yakura.html",
    "title": "yakura replique",
    "section": "",
    "text": "we started replicating the workflow presented in above study beginning with creating the target corpus. Yakura et al. (2025) built corpora from transcribed youtube video audios and podcasts, referencing a research organisations registry (ROR (2025)) to collect videos from institutional educational youtube channels. we assemlbled data the same way restricted to german institutions. Yakura et al. (2025) limited the target corpus to 4 years before the introduction of the GPT chat agent on 2022-11-31 up to 2024-05-31. this allowed a timebased analysis of hypothesised appearance of AI-speech induced variances.\nwe tested the workflow until state of video transcribed using the youtube search API for finding the corresponding video channels, llama3.2 to match the correct channel within the search results, yt-dlp to download the video audio, ffmpeg library to convert to PCM .wav and whisper AI to finally transcribe the audio. all worked well (script) with resulting two texts from Mannheim University channel youtube contributions. we estimate an overall server runtime of about 10h to download and transcribe to text all audio of above categorized channels."
  },
  {
    "objectID": "yakura.html#notes-on-replication-study-of-yakura_empirical_2025",
    "href": "yakura.html#notes-on-replication-study-of-yakura_empirical_2025",
    "title": "yakura replique",
    "section": "",
    "text": "we started replicating the workflow presented in above study beginning with creating the target corpus. Yakura et al. (2025) built corpora from transcribed youtube video audios and podcasts, referencing a research organisations registry (ROR (2025)) to collect videos from institutional educational youtube channels. we assemlbled data the same way restricted to german institutions. Yakura et al. (2025) limited the target corpus to 4 years before the introduction of the GPT chat agent on 2022-11-31 up to 2024-05-31. this allowed a timebased analysis of hypothesised appearance of AI-speech induced variances.\nwe tested the workflow until state of video transcribed using the youtube search API for finding the corresponding video channels, llama3.2 to match the correct channel within the search results, yt-dlp to download the video audio, ffmpeg library to convert to PCM .wav and whisper AI to finally transcribe the audio. all worked well (script) with resulting two texts from Mannheim University channel youtube contributions. we estimate an overall server runtime of about 10h to download and transcribe to text all audio of above categorized channels."
  },
  {
    "objectID": "yakura.html#process-on-yakura-replication",
    "href": "yakura.html#process-on-yakura-replication",
    "title": "yakura replique",
    "section": "2 process on yakura replication",
    "text": "2 process on yakura replication\nafter starting with the same pipeline of building a corpus from youtube material, we decided for another alternative out of resource reasons. the current corpus which we will work on is created from german parliamental protocols, freely available (resource reason) here: DIP (2026). Also we decided for google gemini since working with that model simply costs less than the openAI GPT variant.\nfor the beginning we created a subset of protocols from 2021-01-01 to 2021-07-31 which are 38 protocols. to get the model preferred vocabulary we prompted gemini to summarize each protocol in its own words with restricting to not using more than 5% of words from the original text and limited to 300 words/summary. See prompt text Section 2.1. we postag the corpus and devise relative lemma frequencies to get the gemini keywords.\n\n2.1 gemini prompt\n\n\n[1] \"System prompt: \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[2] \"You are a member of german parliament. Prepare a summary of the text provided to present at a local community meeting of your party members. Output in german language, no preamble, no extra information, just the plain text. Wordcount maximal 300 words, containing not more than 5% of the keywords of the text provided and explicitly not just a list of keywords but an entertaining text. You are supposed to interprete freely, including background insights on daily politics. Keep in mind thatthe text will be used as is as keynotes to the talk being held to the locals. \"\n[3] \"Text:\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n\n\n\nplease preview/follow paper here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "germanic languages, project drafts",
    "section": "",
    "text": "1 index\n\n\n\n\n\nid\n1\n\n\nclass\nVLSG: germanische sprachen im vergleich\n\n\ntutor\nHüning/Konvicna\n\n\nterm\nFUB WS25/26\n\n\nsubject\nclass project drafts\n\n\nabstract\nNA"
  },
  {
    "objectID": "snc.html",
    "href": "snc.html",
    "title": "1 SNC",
    "section": "",
    "text": "1 SNC\n\nsnc.1.2.2.3.4,cache R,docker wks,render in container wks.,new build,syntax,5,wks."
  },
  {
    "objectID": "draft-C.html",
    "href": "draft-C.html",
    "title": "English L1/L2 structures in german >L1 speakers",
    "section": "",
    "text": "still empty."
  },
  {
    "objectID": "draft-C.html#abstract",
    "href": "draft-C.html#abstract",
    "title": "English L1/L2 structures in german >L1 speakers",
    "section": "",
    "text": "still empty."
  }
]
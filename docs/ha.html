<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 15303.ha.draft | xtitle: coherence &amp; presuppositions observations in :schizophrenia: threads</title>
  <meta name="description" content="SPUND-LX - AVL - alii :: appendix &amp; pub essais" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="2 15303.ha.draft | xtitle: coherence &amp; presuppositions observations in :schizophrenia: threads" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="SPUND-LX - AVL - alii :: appendix &amp; pub essais" />
  <meta name="github-repo" content="esteeschwarz/SPUND-LX/tree/main/psych" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 15303.ha.draft | xtitle: coherence &amp; presuppositions observations in :schizophrenia: threads" />
  
  <meta name="twitter:description" content="SPUND-LX - AVL - alii :: appendix &amp; pub essais" />
  

<meta name="author" content="st. schwarz" />


<meta name="date" content="2025-10-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="appendix.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">snc.1</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> index</a></li>
<li class="chapter" data-level="2" data-path="ha.html"><a href="ha.html"><i class="fa fa-check"></i><b>2</b> 15303.ha.draft</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ha.html"><a href="ha.html#subject"><i class="fa fa-check"></i><b>2.1</b> subject</a></li>
<li class="chapter" data-level="2.2" data-path="ha.html"><a href="ha.html#definitions-terminology-assumptions"><i class="fa fa-check"></i><b>2.2</b> definitions, terminology, assumptions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ha.html"><a href="ha.html#coherence"><i class="fa fa-check"></i><b>2.2.1</b> coherence</a></li>
<li class="chapter" data-level="2.2.2" data-path="ha.html"><a href="ha.html#premises"><i class="fa fa-check"></i><b>2.2.2</b> premises</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ha.html"><a href="ha.html#questions"><i class="fa fa-check"></i><b>2.3</b> questions</a></li>
<li class="chapter" data-level="2.4" data-path="ha.html"><a href="ha.html#data"><i class="fa fa-check"></i><b>2.4</b> data</a></li>
<li class="chapter" data-level="2.5" data-path="ha.html"><a href="ha.html#methods"><i class="fa fa-check"></i><b>2.5</b> methods</a></li>
<li class="chapter" data-level="2.6" data-path="ha.html"><a href="ha.html#reflections"><i class="fa fa-check"></i><b>2.6</b> reflections</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="ha.html"><a href="ha.html#range"><i class="fa fa-check"></i><b>2.6.1</b> range</a></li>
<li class="chapter" data-level="2.6.2" data-path="ha.html"><a href="ha.html#author-trace-id"><i class="fa fa-check"></i><b>2.6.2</b> author trace id</a></li>
<li class="chapter" data-level="2.6.3" data-path="ha.html"><a href="ha.html#lexical-diversity"><i class="fa fa-check"></i><b>2.6.3</b> lexical diversity</a></li>
<li class="chapter" data-level="2.6.4" data-path="ha.html"><a href="ha.html#semantics-word-field-embeddings"><i class="fa fa-check"></i><b>2.6.4</b> semantics, word field, embeddings</a></li>
<li class="chapter" data-level="2.6.5" data-path="ha.html"><a href="ha.html#statistics"><i class="fa fa-check"></i><b>2.6.5</b> statistics</a></li>
<li class="chapter" data-level="2.6.6" data-path="ha.html"><a href="ha.html#caveats"><i class="fa fa-check"></i><b>2.6.6</b> caveats</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="ha.html"><a href="ha.html#model-evaluations"><i class="fa fa-check"></i><b>2.7</b> model evaluations</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="ha.html"><a href="ha.html#covariances"><i class="fa fa-check"></i><b>2.7.1</b> covariances</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="ha.html"><a href="ha.html#ref"><i class="fa fa-check"></i><b>2.8</b> REF</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3</b> appendix</a>
<ul>
<li class="chapter" data-level="3.1" data-path="appendix.html"><a href="appendix.html#citetest-method-m13"><i class="fa fa-check"></i><b>3.1</b> citetest, method (M13)</a></li>
<li class="chapter" data-level="3.2" data-path="appendix.html"><a href="appendix.html#legende"><i class="fa fa-check"></i><b>3.2</b> legende</a></li>
<li class="chapter" data-level="3.3" data-path="appendix.html"><a href="appendix.html#anova-analysis"><i class="fa fa-check"></i><b>3.3</b> anova analysis</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="appendix.html"><a href="appendix.html#anova-plain"><i class="fa fa-check"></i><b>3.3.1</b> anova plain</a></li>
<li class="chapter" data-level="3.3.2" data-path="appendix.html"><a href="appendix.html#anova-of-linear-regression-model"><i class="fa fa-check"></i><b>3.3.2</b> anova of linear regression model</a></li>
<li class="chapter" data-level="3.3.3" data-path="appendix.html"><a href="appendix.html#linear-regression-coefficients"><i class="fa fa-check"></i><b>3.3.3</b> linear regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="appendix.html"><a href="appendix.html#plots"><i class="fa fa-check"></i><b>3.4</b> plots</a></li>
<li class="chapter" data-level="3.5" data-path="appendix.html"><a href="appendix.html#ref-1"><i class="fa fa-check"></i><b>3.5</b> REF</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">xtitle: coherence &amp; presuppositions observations in :schizophrenia: threads</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ha.draft" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> 15303.ha.draft<a href="ha.html#ha.draft" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="subject" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> subject<a href="ha.html#subject" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this paper we want to explore <strong>reference marking, coherence and information structure in schizophrenia language</strong> by measuring distance of similar nouns preceded by specified determinants.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a><br />
Inspired by <span class="citation">(<a href="#ref-zimmerer"><strong>zimmerer?</strong></a>)</span>_deictic_2017 we are interested in observations concerning coherence and propositional statement conditions in schizophrenia language, as these linguistic markers appear underinvestigated in that fields research whilst they seem to play a crucial role within target group language features. (As such seen as asset of thinking or world building capacity which might suffer from linguistic standard deviation within the range of positive symptoms.) There seems to be a lot research done concerning frequency based analyses of how typical patients language might appear and how that language deviates in terms of keywords or word fields, but our interest is more directed onto the structural layer of the language which might not be catched by raw frequencies. In our opionion disturbances on that layer might even be hidden and not to grasp easily such that a listener would not always be able to precisely declare what the disturbing factor is. Missing <strong>coherence</strong>, which we will investigate, may be a too narrow explanation to many impressions that schizophrene language leaves the listener with. But it seems to be a good starting point to unveiling structural patterns of patients language.</p>
</div>
<div id="definitions-terminology-assumptions" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> definitions, terminology, assumptions<a href="ha.html#definitions-terminology-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="coherence" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> coherence<a href="ha.html#coherence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several preliminary affordances to a successful communication. One is the <em>coherence</em> of a <a href="">text = way of communication</a>, which accounts for the partner being able to follow the topic and relate subjects and objects referenced. There can be more or less <em>common</em> references and such, that need to be embedded in context to be understood. The underlying network of informations to create that context is what we call <em>information structure</em> of a text. The level of complexity of that network defines how simple it would be to gather the reference from the given information. We might have to go back many sentences or even infer reference from metaphors or such to be able to understand what is said while in the other case simply recall the subject of the last sentence to get the meaning (reference) of the pronoun in <code>also {she} said thisandthat...</code>. The capacity to imagine or have in mind, what concrete information is accessible to the adressee (what he actually knows or can infer) is key to a successful communication, since factors like common-ness, weltwissen and shared knowledge between adressant and adressee and informations accessible from the text itself vary depending on topic, setting, intimacy of the partners and such. So one cannot always be sure that the information provided is sufficient but the grade to which one can give a correct estimate to this sufficiency should here be a measure for our hypothesis, that the very coherence in disturbed language is deficient which lets an utterance be more difficult to understand within the frame of given information. Now one indicator of coherence we assume is <em>reference distance</em> where according to our hypothesis a larger distance would be observed in places where the adressant overestimates<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> the ability of the partner to follow a reference. That would mean that we find a medium shorter distance between referent and reference in the reference corpus<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and larger distances in the target corpus. The references we are interested in are nouns that appear as anaphors i.e. here as noun analogies. The assumption is that if a noun is repeated <em>and</em> is combinded with certain preceding determiners, the speaker assumes that the adressee has some knowledge of what is talked about, depending on the strength of the determination. So e.g. <a href="">this, that, those, these</a> would be rather strong determiners requiring that the noun was introduced before; these are four determiners of our 5 conditions as listed below.</p>
</div>
<div id="premises" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> premises<a href="ha.html#premises" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="deictic-anchoring-and-propositional-complexity" class="section level4 hasAnchor" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> deictic anchoring and propositional complexity<a href="ha.html#deictic-anchoring-and-propositional-complexity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="citation">(<a href="#ref-zimmerer"><strong>zimmerer?</strong></a>)</span>_deictic_2017 consider “Deictic anchoring […] an inherent part of the process by which we make references to aspects in the world including entities, events, locations, and time.” and define propositions as being “statements about the world which can be true or false.” They mention, according to <span class="citation">(<a href="#ref-kuperberg"><strong>kuperberg?</strong></a>_language_2010)</span> “that in people with schizophrenia, cortical activity to semantic abnormalities in sentences is particularly small compared to controls if interpretation requires integration of several sentences” which can mean, that patients are not realising if their utterances are somehow disturbed on the semantics level. If “Delusions and thought disorder can be considered disruptions of propositional meaning” then the patients feeling for their stated propositions (required to the adressee) and further the estimation about what he/she can assume as familiar to the adressee can be wrong. Following Klaus Konrad <span class="citation">(<a href="#ref-mishara"><strong>mishara?</strong></a>_klaus_2010)</span> who “described the onset of a delusion as the loss of ability to transcend an experience and see it with the eyes of others” <span class="citation">(<a href="#ref-zimmerer"><strong>zimmerer?</strong></a>)</span>_deictic_2017 assume that “in thought disorder, the ability to express coherent propositions can be severely impaired.” We take that as premise for our research question.</p>
</div>
</div>
</div>
<div id="questions" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> questions<a href="ha.html#questions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Measuring the referent-reference distance which we assume as an indicator for coherence we hope to find empirical evidence for disturbed or not world building capacities within schizophrenia language. Premising that a large noun distance indicates a low reference-referent association we hypothesise that in a language/ToM setting where the speakers estimation of the audiences context understanding capacities is disturbed we will find higer medium scores for the distance under matching conditions. An environment which has potential to test our hypothesis is the reddit thread r/schizophrenia. As reference corpus we chose reddit r/unpopularopinion. The distance measured should give us information structural evidence of how strong the noun occurences<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> are connected, i.e. if a noun appears out of the blue mostly or if it somewhere before has been introduced to the audience and thus would be more or less legitimated to be determined by an antecedent. Our basic assumptions rely on the <em>taxonomy of given end new information</em> coined by <span class="citation">(<a href="#ref-prince"><strong>prince?</strong></a>)</span>_toward_1981. She develops a hierarchy of references<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> with specific relations to each other, where each item is attributed in terms of <em>familiarity</em><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>, that defines ranges of 1. givennes in the sense of predictability/recoverability, 2. givenness in the sense of saliency, 3. givenness in the sense of “shared knowledge”. (cf. <span class="citation">(<a href="#ref-prince"><strong>prince?</strong></a>)</span>_toward_1981, pp. 226) We base our hypothesis of <em>reference distance as indicator for coherence</em> on this model assuming that the reference/association strength<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> determines the level of text coherence.</p>
</div>
<div id="data" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> data<a href="ha.html#data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We built a corpus of the reddit r/schizophrenia thread (<code>n=1500371 tokens</code>) and a reference corpus of r/unpopularopinion (<code>n=980731 tokens</code>). Both were pos-tagged using the R udpipe package (<span class="citation">Wijffels (<a href="#ref-wijffels_udpipe_2023">2023</a>)</span>) which tags according to the universal dependencies tagset maintained by <span class="citation">De Marneffe et al. (<a href="#ref-de_marneffe_universal_2021">2021</a>)</span>. Still the available data can only, within the pipeline of steadily growing the corpus and devising the noun distances developed be just a starting point from where with more datapoints statistical evaluation becomes relevant.<br />
The dataframe used for our model (actual: M13) consists of <code>142321</code> distance datapoints (sample Tab.X below) derived from the postagged corpus. Because the ranges of the url threads vary heavily between target and reference corpus, the distances are (in M13) normalised to the target corpus (cf. Tab.X for the raw vs. normalised distances comparison.) Outliers are excluded from the analysis since they very probably do not fulfill to can be counted as anaphoric references.</p>
<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:auto; overflow-x: scroll; width:100%; ">
<table class="table" style="width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
token
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
upos
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
target
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
pos
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
prepos
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
url_id
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
range
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
q
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
det
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
aut_id
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
total_mentions
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
dist
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
embed.score
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
dist_rel_within
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
dist_rel_all
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
dist_rel_obs
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
dist_rel_ref
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
state
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
ref
</td>
<td style="text-align:right;">
493122
</td>
<td style="text-align:left;">
PUNCT
</td>
<td style="text-align:right;">
2047
</td>
<td style="text-align:right;">
5198
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
7662
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
0.373
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
41
</td>
</tr>
<tr>
<td style="text-align:left;">
lot
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
obs
</td>
<td style="text-align:right;">
333532
</td>
<td style="text-align:left;">
DET
</td>
<td style="text-align:right;">
559
</td>
<td style="text-align:right;">
6036
</td>
<td style="text-align:left;">
d
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
544
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
184
</td>
<td style="text-align:right;">
0.310
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
98
</td>
<td style="text-align:right;">
58
</td>
<td style="text-align:right;">
137
</td>
</tr>
<tr>
<td style="text-align:left;">
subscriptions
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
ref
</td>
<td style="text-align:right;">
166916
</td>
<td style="text-align:left;">
PART
</td>
<td style="text-align:right;">
1926
</td>
<td style="text-align:right;">
4185
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4984
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
765
</td>
<td style="text-align:right;">
0.469
</td>
<td style="text-align:right;">
820
</td>
<td style="text-align:right;">
586
</td>
<td style="text-align:right;">
348
</td>
<td style="text-align:right;">
820
</td>
</tr>
<tr>
<td style="text-align:left;">
appetite
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
obs
</td>
<td style="text-align:right;">
943941
</td>
<td style="text-align:left;">
PRON
</td>
<td style="text-align:right;">
1466
</td>
<td style="text-align:right;">
547
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
224
</td>
<td style="text-align:right;">
0.430
</td>
<td style="text-align:right;">
779
</td>
<td style="text-align:right;">
1312
</td>
<td style="text-align:right;">
779
</td>
<td style="text-align:right;">
1837
</td>
</tr>
<tr>
<td style="text-align:left;">
people
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
obs
</td>
<td style="text-align:right;">
817645
</td>
<td style="text-align:left;">
VERB
</td>
<td style="text-align:right;">
1267
</td>
<td style="text-align:right;">
787
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
124
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
0.420
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
183
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
256
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
obs
</td>
<td style="text-align:right;">
262276
</td>
<td style="text-align:left;">
DET
</td>
<td style="text-align:right;">
465
</td>
<td style="text-align:right;">
2113
</td>
<td style="text-align:left;">
d
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
470
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
0.329
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
64
</td>
</tr>
<tr>
<td style="text-align:left;">
day
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
obs
</td>
<td style="text-align:right;">
219934
</td>
<td style="text-align:left;">
ADJ
</td>
<td style="text-align:right;">
405
</td>
<td style="text-align:right;">
446
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
236
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
236
</td>
<td style="text-align:right;">
0.387
</td>
<td style="text-align:right;">
1007
</td>
<td style="text-align:right;">
1696
</td>
<td style="text-align:right;">
1007
</td>
<td style="text-align:right;">
2374
</td>
</tr>
<tr>
<td style="text-align:left;">
steak
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
ref
</td>
<td style="text-align:right;">
800737
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:right;">
2237
</td>
<td style="text-align:right;">
13823
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
9749
</td>
<td style="text-align:right;">
370
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
0.606
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
subreddit
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
ref
</td>
<td style="text-align:right;">
145516
</td>
<td style="text-align:left;">
VERB
</td>
<td style="text-align:right;">
1916
</td>
<td style="text-align:right;">
1045
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
82
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
59
</td>
<td style="text-align:right;">
0.291
</td>
<td style="text-align:right;">
253
</td>
<td style="text-align:right;">
181
</td>
<td style="text-align:right;">
107
</td>
<td style="text-align:right;">
253
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:left;">
NOUN
</td>
<td style="text-align:left;">
ref
</td>
<td style="text-align:right;">
385838
</td>
<td style="text-align:left;">
PART
</td>
<td style="text-align:right;">
2010
</td>
<td style="text-align:right;">
13678
</td>
<td style="text-align:left;">
a
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3271
</td>
<td style="text-align:right;">
30
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
0.306
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
10
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="methods" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> methods<a href="ha.html#methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To compute distances we queried the corpus for matching conditions where certain (probable) determiners appear before analogue nouns (anaphors). For each datapoint we collect variables as:</p>
<ul>
<li>thread url</li>
<li>author (anonymised)</li>
<li>thread length (tokens)</li>
<li>lexical diversity (type/token ratio)</li>
<li>lemma</li>
<li>distance (to the preceding occurence, e.g. for three occurences of <a href="">dog</a> we collect 2 distance datapoints)</li>
</ul>
<p>The main function to determine the distances runs on a subset of the corpus with only including all nouns and their position in the corpus. It finds all duplicated nouns per url thread and computes their distances by token position.</p>
</div>
<div id="reflections" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> reflections<a href="ha.html#reflections" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="range" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> range<a href="ha.html#range" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Evaluating with a growing corpus and (reaching up to M[odel]12 with our methods of computing distances) we interestingly find our basic hypothesis tested again, showing an overall larger distance of analogue nouns within the range of 1 thread url for the target corpus. While until M7 we devised distances from a manually assigned url identifier we saw the necessity to define our “range of interest” according to the original http url of the thread, since with a growing corpus the old url ids - derived from the get_thread_url() method of the redditExtractoR package (<span class="citation">(<a href="#ref-rivera"><strong>rivera?</strong></a>)</span>_redditextractor_2023) used for fetching the reddit content - there a no new url ids created since one url fetch gets each time always only around 1000 urls. To ensure unique url ranges within the corpus we as assigned the range (within which the noun distance is calculated) to the real thread url. The corpus itself is after each fetch sorted after url and timestamp so it represents the real flow of conversation within one thread which is important since our distance model is based on the token distances within that thread, so they should follow their natural occurence in time.<br />
The url range is an important variable which we used for normalising the distance values since the mean distances could also depend on the overall thread length. For that we calculated for each normalisation method as are 1. per target, 2. within target and 3. cross target a range factor by which the distance values are divided. The final regression model posits fixed effects of condition, target, det, range and embed score (where target, condition and det are interacting) and random effects of the url_id.</p>
</div>
<div id="author-trace-id" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> author trace id<a href="ha.html#author-trace-id" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another new feature in M11 is the aut_id variable which represents the comment author and is unique to that. In the base .sqlite database the authors are already anonymised, so there should be no way from the published data back to the original author name of the comment. And as expected, including aut_id as random effect in the linear regression model, the significance level for the covariables of interest as are</p>
<ol style="list-style-type: decimal">
<li>q = the condition matching of the noun-preceding token</li>
<li>det = wether that match has postag “DET”</li>
<li>target = obs or reference corpus</li>
</ol>
<p>finally increases.</p>
</div>
<div id="lexical-diversity" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> lexical diversity<a href="ha.html#lexical-diversity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We thought about some serious caveats within the latest method: If (lucky for our hypothesis) the target corpus has significantly higher distance scores over nearly all conditions, does that automatically indicate a less coherent reference-referent association within what is expressed in the comments? Couldn’t we also assume that if the analogue nouns appear more distanced in general that a topic which is including these nouns is simply expanding over a wider range i.e. timeframe? What does that do to our assumptions in terms of coherence? A good way here could be to integrate (from M3) a general lexical diversity factor per url as fixed effect because we can assume that a higher type/token ratio logically decreases the probability of a noun appearing multiple times within a range and we could take that effect into account.</p>
</div>
<div id="semantics-word-field-embeddings" class="section level3 hasAnchor" number="2.6.4">
<h3><span class="header-section-number">2.6.4</span> semantics, word field, embeddings<a href="ha.html#semantics-word-field-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Further we created another covariable possible to integrate in the evaluation model: The semantic embedding of one specific noun appearing on its specific position in the thread range, computed with help of an open LL word embedding model (<span class="citation">(<a href="#ref-nussbaum"><strong>nussbaum?</strong></a>)</span>_nomic_2024.) This is a common AI way of devising semantic relations in a corpus which exceeds a just frequency based keyword analysis. Using an LLM here allows for a distinctive identification of world field embeddings of the noun in question. In that way we get another variable linguistic feature extracted which may give general insights into the level of standardisation that applies to the corpora. So if a noun is found to be embedded with a high score into its context (the url thread) then it can be very much expected to be found there and appears less out-of-context.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
</div>
<div id="statistics" class="section level3 hasAnchor" number="2.6.5">
<h3><span class="header-section-number">2.6.5</span> statistics<a href="ha.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this context we thought about what it means statistically, if a high-score embedded word also ranks high in (distance) significance i.e. generally what the relations of the covariates in the context of the linear regression evaluation express. Let us picture this:</p>
<ol style="list-style-type: decimal">
<li>a word receives a high embed score if it is highly semantically related to the context within which it appears, here the comment thread.</li>
<li>therefore the necessity to introduce/elaborate on it sinks, since it may be considered a “known” or “inferable” entity within the context given.</li>
<li>now if a person is using this word, the determined use appears less incoherent by itself.</li>
<li>the reference distance thus may increase without losing in coherence.</li>
<li><strong>conclusion:</strong> if we for our linear regression use a (base) formula like <code>distance ~ corpus </code> , a continuos <code>embed_score</code> predictor between <code>-1 and 1</code> should correlate positive with the estimates for <code>dist</code> if applied correctly, nestcepas?</li>
</ol>
</div>
<div id="caveats" class="section level3 hasAnchor" number="2.6.6">
<h3><span class="header-section-number">2.6.6</span> caveats<a href="ha.html#caveats" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since devising the word embed score does take much computing ressources we had a script run on a server that solves the computing. But the first essai to integrate the new var into the evaluation model failed due to levels &lt; 2. Why? Because since we ran the script over the complete url ranges in the corpus and that is sorted after target,<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> we did not compute any values for the reference corpus. So we learned this way again on linear regression models which require that a variable has more than one level (which would not be the case if the lmer() function excludes all NA rows: there would be no observations left with target=ref since all its embed.score values are NA and so all target.ref rows will be removed during regression.)</p>
</div>
</div>
<div id="model-evaluations" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> model evaluations<a href="ha.html#model-evaluations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="covariances" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> covariances<a href="ha.html#covariances" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Effects of the same direction for target OBS and REF are observed in <code>qc, range</code> (with positive effects in <code>qc</code>) while contrary effects are observed in <code>qb, qd, qe, qf, det, embed.score, qb:det, qd:det</code> (with negative effects in target=obs and vcvs.)<br />
In words:</p>
<ul>
<li>the antecedents <code>the</code> seem to allow a wider distance between referent and reference in both target=OBS and target=REF.</li>
<li>the antecedents <code>this,that,these,those - my - your,their,his,her</code> decrease distance in target=OBS and increase distance values in target=REF; condition d (<code>a,an,some,any</code>) vcvs.<br />
</li>
<li>higher <code>embed.score</code> values (better embedded noun) decrease distance in target=OBS and increase distance values in target=REF. (cf. par 3.7.5.4, better embedding allows wider distance &gt; the expectation seems only valid for the reference corpus!)</li>
</ul>
<p><strong><em>sidenote:</em></strong> Positing the url range only as fixed effect instead of normalising the distances still estimates smaller distances for the reference corpus, but with no significance, the only significant difference with that regression formula shows in target=REF under condition e (antecedents: <code>my</code>).</p>
<hr />
</div>
</div>
<div id="ref" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> REF<a href="ha.html#ref" class="anchor-section" aria-label="Anchor link to header"></a></h2>

<style type="text/css">
/*table {
  width: 100% !important;
  
}*/
pre {
border: 1px solid black;
border-radius: 0.25rem;
background-color: rgba(0, 0, 0, 0.04);

}
</style>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="ha.html#cb2-1" tabindex="-1"></a><span class="co">#dataset&lt;-7</span></span>
<span id="cb2-2"><a href="ha.html#cb2-2" tabindex="-1"></a><span class="co">#poster-ext</span></span></code></pre></div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-de_marneffe_universal_2021" class="csl-entry">
De Marneffe, Marie-Catherine, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. 2021. <span>“Universal <span>Dependencies</span>.”</span> <em>Computational Linguistics</em>, May, 1–54. <a href="https://doi.org/10.1162/coli_a_00402">https://doi.org/10.1162/coli_a_00402</a>.
</div>
<div id="ref-wijffels_udpipe_2023" class="csl-entry">
Wijffels, Jan. 2023. <em>Udpipe: <span>Tokenization</span>, <span>Parts</span> of <span>Speech</span> <span>Tagging</span>, <span>Lemmatization</span> and <span>Dependency</span> <span>Parsing</span> with the ’<span>UDPipe</span>’ ’<span>NLP</span>’ <span>Toolkit</span></em>. <a href="https://CRAN.R-project.org/package=udpipe">https://CRAN.R-project.org/package=udpipe</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>only according to the LLM training data, which is still a blackbox<a href="ha.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>where “obs” comes first<a href="ha.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>where the participants may show a more realistic estimation of beforementioned ability<a href="ha.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>preceded by conditioned determiners<a href="ha.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>informations in a text<a href="ha.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>cf. Prince: speaker assumptions about hearer familiarity = assumed familiarity<a href="ha.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>which should be weaker with growing distance between reference-referent<a href="ha.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>only according to the LLM training data, which is still a blackbox<a href="ha.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>where “obs” comes first<a href="ha.html#fnref9" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": false,
    "linkedin": true,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["spund-pub.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "github-repo": "esteeschwarz/SPUND-LX",
  "repo-url": "esteeschwarz/SPUND-LX"
});
});
</script>

</body>

</html>
